# Regularization Techniques for Better Performance of NeuralÂ Network

This repository contains the code for analyzing how regularization helps to address the problem of overfitting in deep neural network.

## Outline of the Notebook
1. L2 norm and loading datasets
2. Modifying our FF Class
3. Use more complex models to reduce bias
4. Demonstrate overfitting for very large models
5. Use L2 regularisation for preventing overfitting
6. Use noise on training data for preventing overfitting
7. Exercises


## Jump into code

* Click here to execute the code directly in colab

  [![Click here to open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Niranjankumar-c/DeepLearning-PadhAI/blob/master/DeepLearning_Materials/2_OverFitting_Regularization_NeuralNetworks/OverfittingAndRegularisation.ipynb)

## Blog posts
Related blog posts for better understanding of the code in this repository:
* [Deep Learning Best Practices: Activation Functions & Weight Initialization Methods](https://medium.com/datadriveninvestor/deep-learning-best-practices-activation-functions-weight-initialization-methods-part-1-c235ff976ed)
* [Regularization Techniques for Better Neural Network Performance](https://heartbeat.fritz.ai/deep-learning-best-practices-regularization-techniques-for-better-performance-of-neural-network-94f978a4e518)
