{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59b69284-9fb3-4c5d-934b-1166f2a62f26",
   "metadata": {},
   "source": [
    "# <center>Machine Learning with Python</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a06c2db-2190-4081-9aaa-057f56eb02ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn.model_selection as model_selection\n",
    "import sklearn.preprocessing as preprocessing\n",
    "import sklearn.neighbors as neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f0be2b-58fa-4be4-8dc7-c9e1a31205cb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "---\n",
    "# Introduction\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ab3bc2-c515-43cb-9a50-4bc490673b6b",
   "metadata": {},
   "source": [
    "Machine learning is about <u>extracting knowledge from data</u>. It is a research field at the intersection of <u>statistics, artificial intelligence, and computer science</u> and is also known as <u>predictive analytics or statistical learning</u>. The application of machine learning methods has in recent years become ubiquitous (found everywhere) in everyday life.</br>\n",
    "1. From auto‐ matic recommendations of which movies to watch, to what food to order or which products to buy, to personalized online radio and recognizing your friends in your photos, many modern websites and devices have machine learning algorithms at their core.\n",
    "2. When you look at a complex website like Facebook, Amazon, or Netflix, it is very likely that every part of the site contains multiple machine learning models.</br>\n",
    "\n",
    "Outside of commercial applications, machine learning has had a tremendous influence on the way data-driven research is done today. The tools introduced in this book have been applied to diverse scientific problems such as <u>understanding stars</u>, <u>finding distant planets</u>, <u>discovering new particles</u>, <u>analyzing DNA sequences</u>, and <u>providing personalized cancer treatments</u>.\n",
    "\n",
    "**Note:** Your application doesn’t need to be as large-scale or world-changing as these exam‐ ples in order to benefit from machine learning, though. In this chapter, we will explain why machine learning has become so popular and discuss what kinds of problems can be solved using machine learning. Then, we will show you how to build your first machine learning model, introducing important concepts along the way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac5d9b3-90af-413b-97e8-2d181e10a051",
   "metadata": {},
   "source": [
    "#### Why Machine Learning?\n",
    "In the early days of “intelligent” applications, many systems used handcoded rules of “if ” and “else” decisions to process data or adjust to user input. Think of a spam filter whose job is to move the appropriate incoming email messages to a spam folder. You could make up a blacklist of words that would result in an email being marked as spam. This would be an example of using an expert-designed rule system to design an “intelligent” application. Manually crafting decision rules is feasible for some applica‐ tions, particularly those in which humans have a good understanding of the process to model. However, using handcoded rules to make decisions has two major disadvantages:\n",
    "1. The logic required to make a decision is specific to a single domain and task. Changing the task even slightly might require a rewrite of the whole system.\n",
    "2. Designing rules requires a deep understanding of how a decision should be made by a human expert.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fec77b3-ac6a-495b-a68b-e572537b2c9b",
   "metadata": {},
   "source": [
    "#### Problems Machine Learning Can Solve\n",
    "The most successful kinds of machine learning algorithms are those that automate decision-making processes by generalizing from known examples. In this setting, which is known as **supervised learning**, the user provides the algorithm with pairs of inputs and desired outputs, and the algorithm finds a way to produce the desired out‐ put given an input. In particular, the algorithm is able to create an output for an input it has never seen before without any help from a human. Going back to our example of spam classification, using machine learning, the user provides the algorithm with a large number of emails (which are the input), together with information about whether any of these emails are spam (which is the desired output). Given a new email, the algorithm will then produce a prediction as to whether the new email is spam.\n",
    "\n",
    "Machine learning algorithms that learn from input/output pairs are called supervised learning algorithms because a “teacher” provides supervision to the algorithms in the form of the desired outputs for each example that they learn from. While creating a dataset of inputs and outputs is often a laborious manual process, supervised learning algorithms are well understood and their performance is easy to measure. If your application can be formulated as a supervised learning problem, and you are able to create a dataset that includes the desired outcome, machine learning will likely be able to solve your problem.\n",
    "\n",
    "Examples of **supervised machine learning** tasks include:\n",
    "\n",
    "1. Identifying the zip code from handwritten digits on an envelope</br>\n",
    "    Here the input is a scan of the handwriting, and the desired output is the actual digits in the zip code. To create a dataset for building a machine learning model, you need to collect many envelopes. Then you can read the zip codes yourself and store the digits as your desired outcomes.\n",
    "\n",
    "2. Determining whether a tumor is benign based on a medical image</br>\n",
    "    Here the input is the image, and the output is whether the tumor is benign. To create a dataset for building a model, you need a database of medical images. You also need an expert opinion, so a doctor needs to look at all of the images and decide which tumors are benign and which are not. It might even be necessary to do additional diagnosis beyond the content of the image to determine whether the tumor in the image is cancerous or not.\n",
    "    \n",
    "3. Detecting fraudulent activity in credit card transactions</br>\n",
    "Here the input is a record of the credit card transaction, and the output is whether it is likely to be fraudulent or not. Assuming that you are the entity dis‐ tributing the credit cards, collecting a dataset means storing all transactions and recording if a user reports any transaction as fraudulent.\n",
    "\n",
    "**Note:** An interesting thing to note about these examples is that although the inputs and out‐ puts look fairly straightforward, the data collection process for these three tasks is vastly different. While reading envelopes is laborious, it is easy and cheap. Obtaining medical imaging and diagnoses, on the other hand, requires not only expensive machinery but also rare and expensive expert knowledge, not to mention the ethical concerns and privacy issues. In the example of detecting credit card fraud, data col‐ lection is much simpler. Your customers will provide you with the desired output, as they will report fraud. All you have to do to obtain the input/output pairs of fraudu‐ lent and nonfraudulent activity is wait.</br>\n",
    "\n",
    "**Unsupervised algorithms** are the other type of algorithm that we will cover in this book. <u>In unsupervised learning, only the input data is known, and no known output data is given to the algorithm.</u> While there are many successful applications of these methods, they are usually harder to understand and evaluate.\n",
    "\n",
    "\n",
    "Examples of **unsupervised learning** include:\n",
    "1. Identifying topics in a set of blog posts</br>\n",
    "    If you have a large collection of text data, you might want to summarize it and find prevalent themes in it. You might not know beforehand what these topics are, or how many topics there might be. Therefore, there are no known outputs.\n",
    "\n",
    "2. Segmenting customers into groups with similar preferences</br>\n",
    "Given a set of customer records, you might want to identify which customers are similar, and whether there are groups of customers with similar preferences. For a shopping site, these might be “parents,” “bookworms,” or “gamers.” Because you don’t know in advance what these groups might be, or even how many there are, you have no known outputs.\n",
    "\n",
    "3. Detecting abnormal access patterns to a website</br>\n",
    "To identify abuse or bugs, it is often helpful to find access patterns that are differ‐ ent from the norm. Each abnormal pattern might be very different, and you might not have any recorded instances of abnormal behavior. Because in this example you only observe traffic, and you don’t know what constitutes normal and abnormal behavior, this is an unsupervised problem.\n",
    "\n",
    "\n",
    "For both supervised and unsupervised learning tasks, it is important to have a repre‐ sentation of your input data that a computer can understand. Often it is helpful to think of your data as a table. Each data point that you want to reason about (each email, each customer, each transaction) is a row, and each property that describes that data point (say, the age of a customer or the amount or location of a transaction) is a column. You might describe users by their age, their gender, when they created an account, and how often they have bought from your online shop. You might describe the image of a tumor by the grayscale values of each pixel, or maybe by using the size, shape, and color of the tumor.\n",
    "\n",
    "`Each entity or row here is known as a sample (or data point) in machine learning, while the columns—the properties that describe these entities—are called features.\n",
    "`\n",
    "\n",
    "Later in this book we will go into more detail on the topic of building a good repre‐ sentation of your data, which is called feature extraction or feature engineering. You should keep in mind, however, that no machine learning algorithm will be able to make a prediction on data for which it has no information. For example, if the only feature that you have for a patient is their last name, no algorithm will be able to pre‐ dict their gender. This information is simply not contained in your data. If you add another feature that contains the patient’s first name, you will have much better luck, as it is often possible to tell the gender by a person’s first name.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02823d2-ea3a-48de-acf3-084cd7c6a8d2",
   "metadata": {},
   "source": [
    "#### Knowing Your Task and Knowing Your Data\n",
    "Quite possibly the most important part in the machine learning process is under‐ standing the data you are working with and how it relates to the task you want to solve. It will not be effective to randomly choose an algorithm and throw your data at it. It is necessary to understand what is going on in your dataset before you begin building a model. Each algorithm is different in terms of what kind of data and what problem setting it works best for. While you are building a machine learning solution, you should answer, or at least keep in mind, the following questions:\n",
    "1. What question(s) am I trying to answer? Do I think the data collected can answer that question?\n",
    "2. What is the best way to phrase my question(s) as a machine learning problem?\n",
    "3. Have I collected enough data to represent the problem I want to solve?\n",
    "4. What features of the data did I extract, and will these enable the right predictions?\n",
    "5. How will I measure success in my application?\n",
    "6. How will the machine learning solution interact with other parts of my research or business product?\n",
    "\n",
    "**Note:** In a larger context, the algorithms and methods in machine learning are only one part of a greater process to solve a particular problem, and it is good to keep the big picture in mind at all times. Many people spend a lot of time building complex machine learning solutions, only to find out they don’t solve the right problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fd51fc-bcd8-462b-b4e9-e95af3fa424a",
   "metadata": {},
   "source": [
    "#### Jupyter Notebook\n",
    "The Jupyter Notebook is an interactive environment for running code in the browser. It is a great tool for exploratory data analysis and is widely used by data scientists. While the Jupyter Notebook supports many programming languages, we only need the Python support. The Jupyter Notebook makes it easy to incorporate code, text, and images, and all of this book was in fact written as a Jupyter Notebook. All of the code examples we include can be downloaded from GitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362ecf82-065b-4036-9491-4724d6c28fdf",
   "metadata": {},
   "source": [
    "#### NumPy\n",
    "NumPy is one of the fundamental packages for scientific computing in Python. It contains functionality for multidimensional arrays, high-level mathematical func‐ tions such as linear algebra operations and the Fourier transform, and pseudorandom number generators.\n",
    "\n",
    "\n",
    "<u>In scikit-learn, the NumPy array is the fundamental data structure. scikit-learn takes in data in the form of NumPy arrays.</u> Any data you’re using will have to be con‐ verted to a NumPy array. The core functionality of NumPy is the ndarray class, a multidimensional (n-dimensional) array. All elements of the array must be of the same type.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75eba008-3867-4035-ad76-e5d46ca0f011",
   "metadata": {},
   "source": [
    "#### SciPy\n",
    "SciPy is a collection of functions for scientific computing in Python. It provides, among other functionality, advanced linear algebra routines, mathematical function optimization, signal processing, special mathematical functions, and statistical distri‐ butions. scikit-learn draws from SciPy’s collection of functions for implementing its algorithms. The most important part of SciPy for us is scipy.sparse: this provides sparse matrices, which are another representation that is used for data in scikit- learn. Sparse matrices are used whenever we want to store a 2D array that contains mostly zeros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d40be2-719d-4669-a144-54572aa328fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sparse\n",
    "import numpy as np\n",
    "\n",
    "# Create a 2D NumPy array with a diagonal of ones, and zeros everywhere else\n",
    "eye = np.eye(4)\n",
    "print(f\"NumPy array:\\n{eye}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152d5212-df4b-4810-91d6-6511d90c5e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the NumPy array to a SciPy sparse matrix in CSR format # Only the nonzero entries are stored\n",
    "sparse_matrix = sparse.csr_matrix(eye)\n",
    "print(f\"\\nSciPy sparse CSR matrix:\\n{sparse_matrix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a910a95c-f723-4867-ba73-ac4f3084b40b",
   "metadata": {},
   "source": [
    "More details on SciPy sparse matrices can be found in the [SciPy Lecture Notes](http://www.scipy-lectures.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be859add-5fac-49fb-b8e7-c7ff3bee9a56",
   "metadata": {},
   "source": [
    "#### matplotlib\n",
    "matplotlib is the primary scientific plotting library in Python. It provides functions for making publication-quality visualizations such as line charts, histograms, scatter plots, and so on. Visualizing your data and different aspects of your analysis can give you important insights, and we will be using matplotlib for all our visualizations. When working inside the Jupyter Notebook, you can show figures directly in the browser by using the %matplotlib notebook and %matplotlib inline commands. We recommend using %matplotlib notebook, which provides an interactive envi‐ ronment (though we are using %matplotlib inline to produce this book). For example, this code produces the plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef91641-940b-4d5b-b919-ae9300f12ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib notebook\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate a sequence of numbers from -10 to 10 with 100 steps in between\n",
    "x = np.linspace(-10,10,100)\n",
    "y = np.sin(x)\n",
    "\n",
    "# The plot function makes a line chart of one array against another\n",
    "plt.plot(x,y, marker = \"x\")\n",
    "plt.grid(axis=\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040b05ba-dd47-435d-b661-7e597ea5ebb6",
   "metadata": {},
   "source": [
    "#### pandas\n",
    "pandas is a Python library for data wrangling and analysis. It is built around a data structure called the DataFrame that is modeled after the R DataFrame. Simply put, a pandas DataFrame is a table, similar to an Excel spreadsheet. pandas provides a great range of methods to modify and operate on this table; in particular, it allows SQL-like queries and joins of tables. In contrast to NumPy, which requires that all entries in an array be of the same type, pandas allows each column to have a separate type (for example, integers, dates, floating-point numbers, and strings). Another valuable tool provided by pandas is its ability to ingest from a great variety of file formats and data‐ bases, like SQL, Excel files, and comma-separated values (CSV) files. Going into detail about the functionality of pandas is out of the scope of this book. However, Python for Data Analysis by Wes McKinney (O’Reilly, 2012) provides a great guide. Here is a small example of creating a DataFrame using a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879a77f1-1081-407f-a0f5-83caafd9b714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a simple dataset of people\n",
    "data = {\n",
    "    \"name\":[\"subrata\",\"mrinal\",\"bapai\"],\n",
    "    \"location\":[\"jangipur\",\"malda\",\"gazole\"],\n",
    "    \"degree\":[\"btech-cse\",\"btech-ece\",\"arts\"],\n",
    "    \"fake age\":[22,18,31]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "# IPython.display allows \"pretty printing\" of dataframes # in the Jupyter notebook\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123992ee-4235-4d7d-93fd-9fb239254896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all rows that have an age column greater than 30\n",
    "display(df[df[\"fake age\"] > 30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc666b65-1a27-4189-9daf-604fa58a72cb",
   "metadata": {},
   "source": [
    "#### mglearn\n",
    "This book comes with accompanying code, which you can find on [GitHub](https://github.com/amueller/introduction_to_ml_with_python). The accompanying code includes not only all the examples shown in this book, but also the mglearn library. This is a library of utility functions we wrote for this book, so that we don’t clutter up our code listings with details of plotting and data loading. If you’re interested, you can look up all the functions in the repository, but the details of the mglearn module are not really important to the material in this book. If you see a call to mglearn in the code, it is usually a way to make a pretty picture quickly, or to get our hands on some interesting data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d99c7c-2582-4745-be81-a6916344ddca",
   "metadata": {},
   "source": [
    "### A First Application: Classifying Iris Species\n",
    "In this section, we will go through a simple machine learning application and create our first model. In the process, we will introduce some core concepts and terms.\n",
    "\n",
    "Let’s assume that a hobby botanist is interested in distinguishing the species of some iris flowers that she has found. She has collected some measurements associated with each iris: the length and width of the petals and the length and width of the sepals, all measured in centimeters.\n",
    "\n",
    "She also has the measurements of some irises that have been previously identified by an expert botanist as belonging to the species setosa, versicolor, or virginica. For these measurements, she can be certain of which species each iris belongs to. Let’s assume that these are the only species our hobby botanist will encounter in the wild.\n",
    "\n",
    "Our goal is to build a machine learning model that can learn from the measurements of these irises whose species is known, so that we can predict the species for a new iris.</br>\n",
    "\n",
    "<img src=\"images/iris.png\"></img>\n",
    "\n",
    "</br>\n",
    "\n",
    "Because we have measurements for which we know the correct species of iris, this is a <u>supervised learning problem</u>. In this problem, we want to predict one of several options (the species of iris). This is an example of a classification problem. The possi‐ ble outputs (different species of irises) are called <u>classes</u>. Every iris in the dataset belongs to one of three classes, so this problem is a <u>three-class classification problem</u>.\n",
    "\n",
    "The desired output for a single data point (an iris) is the species of this flower. For a particular data point, the species it belongs to is called its <u>label</u>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f258f356-dec5-44a7-a220-57f72e7365b1",
   "metadata": {},
   "source": [
    "#### Meet the Data\n",
    "The data we will use for this example is the Iris dataset, a classical dataset in machine learning and statistics. It is included in scikit-learn in the datasets module. We can load it by calling the load_iris function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fe7ad5-9155-4472-8285-14c8b67e612f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "import sklearn.datasets as datasets\n",
    "\n",
    "iris_data = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcf2548-c369-4292-bd9a-77e5c4f1ffaf",
   "metadata": {},
   "source": [
    "The iris object that is returned by load_iris is a Bunch object, which is very similar to a dictionary. It contains keys and values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabf9bac-216a-4723-9feb-b42bc4b6fa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abaa440-8c7b-4cae-bdf9-c01fa6105a55",
   "metadata": {},
   "source": [
    "The value of the key DESCR is a short description of the dataset. We show the begin‐ ning of the description here (feel free to look up the rest yourself):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40c9040-4a9d-4e86-994f-71170a8ee2de",
   "metadata": {},
   "source": [
    "#### Data Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b1081a-417d-4ce9-93cc-96fdc3a29f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris_data[\"DESCR\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb963b2-2169-45df-bedb-0bcc164c8de1",
   "metadata": {},
   "source": [
    ".. _iris_dataset:\n",
    "\n",
    "#### Iris plants dataset\n",
    "--------------------\n",
    "\n",
    "**Data Set Characteristics:**\n",
    "\n",
    "    :Number of Instances: 150 (50 in each of three classes)\n",
    "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
    "    :Attribute Information:\n",
    "        - sepal length in cm\n",
    "        - sepal width in cm\n",
    "        - petal length in cm\n",
    "        - petal width in cm\n",
    "        - class:\n",
    "                - Iris-Setosa\n",
    "                - Iris-Versicolour\n",
    "                - Iris-Virginica\n",
    "                \n",
    "    :Summary Statistics:\n",
    "\n",
    "    ============== ==== ==== ======= ===== ====================\n",
    "                    Min  Max   Mean    SD   Class Correlation\n",
    "    ============== ==== ==== ======= ===== ====================\n",
    "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
    "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
    "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
    "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
    "    ============== ==== ==== ======= ===== ====================\n",
    "\n",
    "    :Missing Attribute Values: None\n",
    "    :Class Distribution: 33.3% for each of 3 classes.\n",
    "    :Creator: R.A. Fisher\n",
    "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
    "    :Date: July, 1988\n",
    "\n",
    "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
    "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
    "Machine Learning Repository, which has two wrong data points.\n",
    "\n",
    "This is perhaps the best known database to be found in the\n",
    "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
    "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
    "data set contains 3 classes of 50 instances each, where each class refers to a\n",
    "type of iris plant.  One class is linearly separable from the other 2; the\n",
    "latter are NOT linearly separable from each other.\n",
    "\n",
    ".. topic:: References\n",
    "\n",
    "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
    "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
    "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
    "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
    "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
    "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
    "     Structure and Classification Rule for Recognition in Partially Exposed\n",
    "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
    "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
    "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
    "     on Information Theory, May 1972, 431-433.\n",
    "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
    "     conceptual clustering system finds 3 classes in the data.\n",
    "   - Many, many more ...\n",
    "   \n",
    "   \n",
    "   The value of the key target_names is an array of strings, containing the species of\n",
    "flower that we want to predict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573f2bcf-0537-4b54-91b3-62618047be7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target names\n",
    "iris_data[\"target_names\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37241717-7aef-42b2-a23e-1827c87ace70",
   "metadata": {},
   "source": [
    "The value of feature_names is a list of strings, giving the description of each feature: In[14]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc98c0a-9837-417c-941b-b3b6eb0f9b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data[\"feature_names\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c86097c-c2c1-4b4b-93f3-0c07367049f7",
   "metadata": {},
   "source": [
    "The data itself is contained in the target and data fields. data contains the numeric measurements of sepal length, sepal width, petal length, and petal width in a NumPy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ae5100-6ae5-49d9-b99d-33f1963ea45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data[\"data\"][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d114fa5-5f6c-429a-a7d6-5d9f5e757389",
   "metadata": {},
   "source": [
    "The rows in the data array correspond to flowers, while the columns represent the\n",
    "four measurements that were taken for each flower:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2206a5fc-d796-4314-9b2c-1124f1ea6a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data[\"data\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e7fe1c-8b43-42ce-b41a-28ba87a895a3",
   "metadata": {},
   "source": [
    "We see that the array contains measurements for 150 different flowers. Remember that the individual items are called samples in machine learning, and their properties are called features. The shape of the data array is the number of samples multiplied by the number of features. This is a convention in scikit-learn, and your data will always be assumed to be in this shape. Here are the feature values for the first five samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e80350-9adb-4751-89c9-106ad50a1bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data[\"data\"][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9db791-0c62-4dc2-8183-11d04c97d46b",
   "metadata": {},
   "source": [
    "From this data, we can see that all of the first five flowers have a petal width of 0.2 cm and that the first flower has the longest sepal, at 5.1 cm.\n",
    "\n",
    "The target array contains the species of each of the flowers that were measured, also as a NumPy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1209d9d1-bcbb-4963-a91b-a15c365139c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae3b98c-a549-4236-9988-5a55194fe089",
   "metadata": {},
   "source": [
    "target is a one-dimensional array, with one entry per flower:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbe6f21-4d37-4961-a8f8-bedc751b48ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data[\"target\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24886e47-f760-44c8-8395-f485191385d7",
   "metadata": {},
   "source": [
    "The species are encoded as integers from 0 to 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d408a5-0493-4613-b8e1-876cc58f9de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bdfe99-ff60-4f83-b436-d5579b24418d",
   "metadata": {},
   "source": [
    "The meanings of the numbers are given by the iris['target_names'] array: 0 means setosa, 1 means versicolor, and 2 means virginica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1eb85c9-0716-42d7-a94a-a6a6e340d09e",
   "metadata": {},
   "source": [
    "#### Measuring Success: Training and Testing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3b5441-27b4-43a9-aea6-b600ab1d278a",
   "metadata": {},
   "source": [
    "We want to build a machine learning model from this data that can predict the spe‐ cies of iris for a new set of measurements. But before we can apply our model to new measurements, we need to know whether it actually works—that is, whether we should trust its predictions.\n",
    "\n",
    "Unfortunately, we cannot use the data we used to build the model to evaluate it. This is because our model can always simply remember the whole training set, and will therefore always predict the correct label for any point in the training set. This “remembering” does not indicate to us whether our model will generalize well (in other words, whether it will also perform well on new data).\n",
    "To assess the model’s performance, we show it new data (data that it hasn’t seen before) for which we have labels. This is usually done by splitting the labeled data we have collected (here, our 150 flower measurements) into two parts. One part of the data is used to build our machine learning model, and is called the training data or training set. The rest of the data will be used to assess how well the model works; this is called the test data, test set, or hold-out set.\n",
    "\n",
    "scikit-learn contains a function that shuffles the dataset and splits it for you: the train_test_split function. This function extracts 75% of the rows in the data as the training set, together with the corresponding labels for this data. The remaining 25% of the data, together with the remaining labels, is declared as the test set. Deciding how much data you want to put into the training and the test set respectively is some‐ what arbitrary, but using a test set containing 25% of the data is a good rule of thumb.\n",
    "In scikit-learn, data is usually denoted with a capital X, while labels are denoted by a lowercase y. This is inspired by the standard formulation f(x)=y in mathematics, where x is the input to a function and y is the output. Following more conventions from mathematics, we use a capital X because the data is a two-dimensional array (a matrix) and a lowercase y because the target is a one-dimensional array (a vector).\n",
    "\n",
    "Let’s call train_test_split on our data and assign the outputs using this nomenclature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758693fa-f559-44e2-8052-d537ea88dc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = model_selection.train_test_split(iris_data[\"data\"],iris_data[\"target\"], test_size=0.2, random_state=444)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cbf8ac-3364-49f8-9281-c7e2b6d27dc9",
   "metadata": {},
   "source": [
    "Before making the split, the train_test_split function shuffles the dataset using a pseudorandom number generator. If we just took the last 25% of the data as a test set, all the data points would have the label 2, as the data points are sorted by the label (see the output for iris['target'] shown earlier). <u>Using a test set containing only one of the three classes would not tell us much about how well our model generalizes, so we shuffle our data to make sure the test data contains data from all classes.</u>\n",
    "\n",
    "<u>To make sure that we will get the same output if we run the same function several times, we provide the pseudorandom number generator with a fixed seed using the random_state parameter.</u> This will make the outcome <u>deterministic</u>, so this line will always have the same outcome. We will always fix the random_state in this way when using randomized procedures in this book.\n",
    "\n",
    "The output of the train_test_split function is X_train, X_test, y_train, and y_test, which are all NumPy arrays. X_train contains 75% of the rows of the dataset, and X_test contains the remaining 25%:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5881268-4fb3-46ee-b7af-8438777660d0",
   "metadata": {},
   "source": [
    "#### First Things First: Look at Your Data\n",
    "Before building a machine learning model it is often a good idea to inspect the data, to see if the task is easily solvable without machine learning, or if the desired infor‐ mation might not be contained in the data.\n",
    "Additionally, inspecting your data is a good way to find abnormalities and peculiari‐ ties. Maybe some of your irises were measured using inches and not centimeters, for example. In the real world, inconsistencies in the data and unexpected measurements are very common.\n",
    "\n",
    "One of the best ways to inspect data is to visualize it. One way to do this is by using a scatter plot. A scatter plot of the data puts one feature along the x-axis and another along the y-axis, and draws a dot for each data point. Unfortunately, computer screens have only two dimensions, which allows us to plot only two (or maybe three) features at a time. It is difficult to plot datasets with more than three features this way. One way around this problem is to do a pair plot, which looks at all possible pairs of features. If you have a small number of features, such as the four we have here, this is quite reasonable. You should keep in mind, however, that a pair plot does not show the interaction of all of features at once, so some interesting aspects of the data may not be revealed when visualizing it this way.\n",
    "\n",
    "**Note:** is a pair plot of the features in the training set. The data points are colored according to the species the iris belongs to. To create the plot, we first convert the NumPy array into a pandas DataFrame. pandas has a function to create pair plots called scatter_matrix. The diagonal of this matrix is filled with histograms of each feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadbdf53-4856-4105-884b-0c4c7bc0597f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe from data in X_train\n",
    "# label the columns using the strings in iris_dataset.feature_names\n",
    "X_df = pd.DataFrame(X_train,columns=iris_data.feature_names)\n",
    "\n",
    "# create a scatter matrix from the dataframe, color by y_train\n",
    "grr = pd.plotting.scatter_matrix(X_df,c=y_train, figsize=(15,15),grid=True,marker=\"o\",hist_kwds={\"bins\":20},s=60, alpha=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c8c687-fd7b-4bfc-87d6-1b49e7c9abf5",
   "metadata": {},
   "source": [
    "From the plots, we can see that the three classes seem to be relatively well separated using the sepal and petal measurements. This means that a machine learning model will likely be able to learn to separate them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a29bc0-8908-47ec-96dc-b9999eb8a6ed",
   "metadata": {},
   "source": [
    "### Building Your First Model: k-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d46b6b-bb8a-4441-ac9b-116762fd2826",
   "metadata": {},
   "source": [
    "Now we can start building the actual machine learning model. There are many classi‐ fication algorithms in scikit-learn that we could use. Here we will use a k-nearest neighbors classifier, which is easy to understand. Building this model only consists of storing the training set. To make a prediction for a new data point, the algorithm finds the point in the training set that is closest to the new point. Then it assigns the label of this training point to the new data point.\n",
    "\n",
    "The k in k-nearest neighbors signifies that instead of using only the closest neighbor to the new data point, we can consider any fixed number k of neighbors in the train‐ ing (for example, the closest three or five neighbors). Then, we can make a prediction using the majority class among these neighbors.\n",
    "\n",
    "<u>All machine learning models in scikit-learn are implemented in their own classes, which are called Estimator classes.</u> The k-nearest neighbors classification algorithm is implemented in the KNeighborsClassifier class in the neighbors module. Before we can use the model, we need to instantiate the class into an object. This is when we will set any parameters of the model. The most important parameter of KNeighbor sClassifier is the number of neighbors, which we will set to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1835d3e6-d7e3-499e-8d2a-90b09b3ecfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.neighbors as neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07f2c9b-a8db-4a9d-be60-7638fece6216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create object\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca479929-278c-4124-995c-3c855b73d4f5",
   "metadata": {},
   "source": [
    "<u>The knn object encapsulates the algorithm that will be used to build the model from the training data</u>, as well the algorithm to make predictions on new data points. <u>It will also hold the information that the algorithm has extracted from the training data</u>. In the case of KNeighborsClassifier, it will just store the training set.\n",
    "\n",
    "To build the model on the training set, we call the fit method of the knn object, which takes as arguments the NumPy array X_train containing the training data and the NumPy array y_train of the corresponding training labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3758d640-75d4-4ef2-8f68-2e490e8ed9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11d9c31-c6a5-4852-9761-0cc1babe7051",
   "metadata": {},
   "source": [
    "The fit method returns the knn object itself (and modifies it in place), so we get a string representation of our classifier. The representation shows us which parameters were used in creating the model. Nearly all of them are the default values, but you can also find n_neighbors=1, which is the parameter that we passed. Most models in scikit-learn have many parameters, but the majority of them are either speed opti‐ mizations or for very special use cases. You don’t have to worry about the other parameters shown in this representation. Printing a scikit-learn model can yield very long strings, but don’t be intimidated by these."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd90d4fe-55ba-49fd-a6cf-b810d2056978",
   "metadata": {},
   "source": [
    "#### Making Predictions\n",
    "We can now make predictions using this model on new data for which we might not know the correct labels. Imagine we found an iris in the wild with a sepal length of 5 cm, a sepal width of 2.9 cm, a petal length of 1 cm, and a petal width of 0.2 cm. What species of iris would this be? We can put this data into a NumPy array, again by calculating the shape—that is, the number of samples (1) multiplied by the number of features (4):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3a95f5-9009-4ccb-90e1-9b07fb11b2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.array([[5, 2.9, 1, 0.2]])\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc1de35-d357-453a-9e70-f54605ffa0be",
   "metadata": {},
   "source": [
    "Note that we made the measurements of this single flower into a row in a two- dimensional NumPy array, as **scikit-learn always expects two-dimensional arrays for the data**.\n",
    "\n",
    "To make a prediction, we call the predict method of the knn object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9290f84b-1b9d-4df9-9396-73287c47e5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = knn.predict(X_new)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d005d606-8f2c-489f-9cce-e57382c8fdc1",
   "metadata": {},
   "source": [
    "Our model predicts that this new iris belongs to the class 0, meaning its species is setosa. But how do we know whether we can trust our model? We don’t know the correct species of this sample, which is the whole point of building the model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ef651f-0891-4aad-bf3c-bdde467197d8",
   "metadata": {},
   "source": [
    "#### Evaluating the Model\n",
    "This is where the test set that we created earlier comes in. This data was not used to build the model, but we do know what the correct species is for each iris in the test set.\n",
    "\n",
    "Therefore, we can make a prediction for each iris in the test data and compare it against its label (the known species). We can measure how well the model works by computing the accuracy, which is the fraction of flowers for which the right species was predicted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb56837-c4bd-4e65-9e7b-0a3276154e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cffc038-deb9-4ea6-8ba1-61b75fd4e174",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Test score: {np.mean(y_test == y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03facc9a-eda5-47bf-afd8-89b3def4c4bf",
   "metadata": {},
   "source": [
    "We can also use the score method of the knn object, which will compute the test set\n",
    "accuracy for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e274d2-2e7a-45ed-a213-f3cc69636eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8c908e-a6e9-4512-a059-5c3ac476f2b2",
   "metadata": {},
   "source": [
    "For this model, the test set accuracy is about 0.93, which means we made the right prediction for 97% of the irises in the test set. Under some mathematical assump‐ tions, this means that we can expect our model to be correct 93% of the time for new irises. For our hobby botanist application, this high level of accuracy means that our model may be trustworthy enough to use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672d8a68-6a8e-4a8f-9e07-c20c886b8905",
   "metadata": {},
   "source": [
    "### Summary and Outlook\n",
    "Let’s summarize what we learned in this chapter. We started with a brief introduction to machine learning and its applications, then discussed the distinction between supervised and unsupervised learning and gave an overview of the tools we’ll be using in this book. Then, we formulated the task of predicting which species of iris a particular flower belongs to by using physical measurements of the flower. We used a dataset of measurements that was annotated by an expert with the correct species to build our model, making this a supervised learning task. There were three possible species, setosa, versicolor, or virginica, which made the task a three-class classification problem. The possible species are called classes in the classification problem, and the species of a single iris is called its label.\n",
    "\n",
    "The Iris dataset consists of two NumPy arrays: one containing the data, which is referred to as X in scikit-learn, and one containing the correct or desired outputs, which is called y. The array X is a two-dimensional array of features, with one row per data point and one column per feature. The array y is a one-dimensional array, which here contains one class label, an integer ranging from 0 to 2, for each of the samples.\n",
    "\n",
    "We split our dataset into a training set, to build our model, and a test set, to evaluate how well our model will generalize to new, previously unseen data.\n",
    "\n",
    "We chose the k-nearest neighbors classification algorithm, which makes predictions for a new data point by considering its closest neighbor(s) in the training set. This is implemented in the KNeighborsClassifier class, which contains the algorithm that builds the model as well as the algorithm that makes a prediction using the model. We instantiated the class, setting parameters. Then we built the model by calling the fit method, passing the training data (X_train) and training outputs (y_train) as parameters. We evaluated the model using the score method, which computes the accuracy of the model. We applied the score method to the test set data and the test set labels and found that our model is about 97% accurate, meaning it is correct 97% of the time on the test set.\n",
    "\n",
    "This gave us the confidence to apply the model to new data (in our example, new flower measurements) and trust that the model will be correct about 97% of the time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523dc5e9-b1e8-402c-ad8e-942cbbc03d25",
   "metadata": {},
   "source": [
    "# Supervised Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723e03d6-2094-45b2-81aa-c577a67f87cb",
   "metadata": {},
   "source": [
    "Remember that <u>supervised learning is used whenever we want to predict a certain outcome from a given input</u>, and we have examples of input/output pairs. We build a machine learning model from these input/output pairs, which comprise our training set. Our goal is to make accurate predictions for new, never-before-seen data. Supervised learning often requires human effort to build the training set, but afterward automates and often speeds up an otherwise laborious or infeasible task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6d96cc-f738-407f-aa9e-491616c81e5c",
   "metadata": {},
   "source": [
    "## Classification and Regression\n",
    "There are two major types of supervised machine learning problems, called **classification** and **regression**.\n",
    "\n",
    "**Classification:** In classification, the goal is to predict a class label, which is a choice from a predefined list of possibilities.\n",
    "\n",
    "Classification is sometimes separated into binary classification, which is the special case of distinguishing between exactly two classes, and multiclass classification, which is classification between more than two classes. You can think of binary classification as trying to answer a yes/no question. Classifying emails as either spam or not spam is an example of a binary classification problem. In this binary classification task, the yes/no question being asked would be “Is this email spam?” The iris example, on the other hand, is an example of a multiclass classification problem.\n",
    "\n",
    "**Regression:** For regression tasks, the goal is to predict a continuous number, or a floating-point number in programming terms (or real number in mathematical terms). Predicting a person’s annual income from their education, their age, and where they live is an example of a regression task. When predicting income, the predicted value is an amount, and can be any number in a given range. Another example of a regression task is predicting the yield of a corn farm given attributes such as previous yields, weather, and number of employees working on the farm. The yield again can be an arbitrary number.\n",
    "\n",
    "An easy way to distinguish between classification and regression tasks is to ask whether there is some kind of continuity in the output. If there is continuity between possible outcomes, then the problem is a regression problem. Think about predicting annual income. There is a clear continuity in the output. Whether a person makes $40,000 or $40,001 a year does not make a tangible difference, even though these are different amounts of money; if our algorithm predicts $39,999 or $40,001 when it should have predicted $40,000, we don’t mind that much.\n",
    "\n",
    "By contrast, for the task of recognizing the language of a website (which is a classifi‐ cation problem), there is no matter of degree. A website is in one language, or it is in another. There is no continuity between languages, and there is no language that is between English and French.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7731b51-51bb-466a-bd2a-f7251d04e747",
   "metadata": {},
   "source": [
    "## Generalization, Overfitting, and Underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660736b0-4831-49be-b770-563a218d0a61",
   "metadata": {},
   "source": [
    "In **supervised learning**, we want to build a model on the training data and then be able to make accurate predictions on new, unseen data that has the same characteristics as the training set that we used to train the model. If a model is able to make accurate predictions on unseen data, we say that the model is able to generalize from the training set to the test set. We want to build a model that is able to generalize as accurately as possible.\n",
    "\n",
    "Usually we build a model in such a way that it can make accurate predictions on the training set. If the training and test sets have enough in common, we expect the model to also be accurate on the test set. However, there are some cases where this can go wrong. For example, if we allow ourselves to build very complex models, we can always be as accurate as we like on the training set.\n",
    "\n",
    "**Overfitting:** Overfitting occurs when you fit a model too closely to the particularities of the training set and obtain a model that works well on the training set but is not able to generalize to new data. \n",
    "**Underfitting:** On the other hand, if your model is too simple, then you might not be able to capture all the aspects of and variability in the data, and your model will do badly even on the training set. Choosing too simple a model is called underfitting.\n",
    "\n",
    "The more complex we allow our model to be, the better we will be able to predict on the training data. However, if our model becomes too complex, we start focusing too much on each individual data point in our training set, and the model will not generalize well to new data.\n",
    "\n",
    "There is a sweet spot in between that will yield the best generalization performance. This is the model we want to find."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ee6d6f-e10c-4a39-9632-bea7ad8db6bd",
   "metadata": {},
   "source": [
    "## Relation of Model Complexity to Dataset Size\n",
    "It’s important to note that model complexity is intimately tied to the variation of inputs contained in your training dataset: the larger variety of data points your data‐ set contains, the more complex a model you can use without overfitting. Usually, col‐ lecting more data points will yield more variety, so larger datasets allow building more complex models. However, simply duplicating the same data points or collect‐ ing very similar data will not help.\n",
    "\n",
    "**Note:** Having more data and building appropriately more complex models can often work wonders for supervised learning tasks. In this book, we will focus on working with datasets of fixed sizes. In the real world, you often have the ability to decide how much data to collect, which might be more beneficial than tweaking and tuning your model. Never underestimate the power of more data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14718d87-15a2-46ac-bc2d-8a879fb3f8bd",
   "metadata": {},
   "source": [
    "## Supervised Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79397dc-55f0-49ea-9752-6fe58b2c5bf8",
   "metadata": {},
   "source": [
    "### Some Sample Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75cf676-4b8c-4724-addb-158937052d72",
   "metadata": {},
   "source": [
    "An example of a synthetic two-class classification dataset is the forge dataset, which has two features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e567d8d-064e-4613-a711-bbde31008bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_forge():\n",
    "    # a carefully hand-designed dataset lol\n",
    "    X, y = make_blobs(centers=2, random_state=4, n_samples=30)\n",
    "    y[np.array([7, 27])] = 0\n",
    "    mask = np.ones(len(X), dtype=np.bool)\n",
    "    mask[np.array([0, 1, 5, 26])] = 0\n",
    "    X, y = X[mask], y[mask]\n",
    "    return X,y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5be0c419-0357-43d0-b259-65304314c5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/deprecation.py:86: FutureWarning: Function make_blobs is deprecated; Please import make_blobs directly from scikit-learn\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/var/folders/61/l2cgm0251bd792ws429rg7ww0000gn/T/ipykernel_10190/2544799220.py:5: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.  (This may have returned Python scalars in past versions.\n",
      "  mask = np.ones(len(X), dtype=np.bool)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'bool'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# generate dataset\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[43mmake_forge\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m, in \u001b[0;36mmake_forge\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m X, y \u001b[38;5;241m=\u001b[39m make_blobs(centers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, n_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m)\n\u001b[1;32m      4\u001b[0m y[np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m27\u001b[39m])] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 5\u001b[0m mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;28mlen\u001b[39m(X), dtype\u001b[38;5;241m=\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbool\u001b[49m)\n\u001b[1;32m      6\u001b[0m mask[np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m26\u001b[39m])] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      7\u001b[0m X, y \u001b[38;5;241m=\u001b[39m X[mask], y[mask]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/numpy/__init__.py:284\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtesting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tester\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Tester\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    285\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;18m__name__\u001b[39m, attr))\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'bool'"
     ]
    }
   ],
   "source": [
    "# generate dataset\n",
    "X, y = make_forge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c953ac-06df-4daa-a100-772639720a3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
