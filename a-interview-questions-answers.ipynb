{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c682086a",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-03-09T08:19:28.912608Z",
     "iopub.status.busy": "2023-03-09T08:19:28.911241Z",
     "iopub.status.idle": "2023-03-09T08:19:28.924557Z",
     "shell.execute_reply": "2023-03-09T08:19:28.923290Z"
    },
    "papermill": {
     "duration": 0.024047,
     "end_time": "2023-03-09T08:19:28.928113",
     "exception": false,
     "start_time": "2023-03-09T08:19:28.904066",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bd0702",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-09T08:05:46.568707Z",
     "iopub.status.busy": "2023-03-09T08:05:46.567725Z",
     "iopub.status.idle": "2023-03-09T08:05:46.576474Z",
     "shell.execute_reply": "2023-03-09T08:05:46.574323Z",
     "shell.execute_reply.started": "2023-03-09T08:05:46.568661Z"
    },
    "papermill": {
     "duration": 0.002829,
     "end_time": "2023-03-09T08:19:28.934308",
     "exception": false,
     "start_time": "2023-03-09T08:19:28.931479",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# What is ARIMA (Auto Regressive Integrated Moving Average) Model ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e697f8b",
   "metadata": {
    "papermill": {
     "duration": 0.002982,
     "end_time": "2023-03-09T08:19:28.940375",
     "exception": false,
     "start_time": "2023-03-09T08:19:28.937393",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "An ARIMA model is a statistical method used for time series forecasting that captures different patterns or features that reflect how the data change over time. For example, temporal structures can include trends, seasonality, cycles, autocorrelation, etc. It is a class of model that uses past values (autoregressive), differences (integrated) and errors (moving average) to predict future values.\n",
    "\n",
    "An example of using an ARIMA model for stock prediction is to forecast the future prices of a stock based on its past performance. This can be done by fitting an ARIMA model to the historical data and then using it to generate forecasts for a given period. \n",
    "\n",
    "There are different ways to choose the parameters of an ARIMA model, such as grid search, auto-arima or AIC/BIC criteria.\n",
    "\n",
    "However, ARIMA models have some limitations when applied to stock prediction, such as assuming linearity, stationarity and constant variance in the data. They also do not account for external factors that may affect the stock prices, such as news, events or market sentiment. Therefore, they may not be very accurate or reliable for long-term forecasting or volatile markets.\n",
    "\n",
    "## Advantages \n",
    "\n",
    "## Disadvantages\n",
    "\n",
    "## How to improve accuracy of ARIMA MODELS?\n",
    "There are different ways to improve the accuracy of ARIMA models, depending on the characteristics of the data and the forecasting goal. \n",
    "\n",
    "Some possible methods are:\n",
    "\n",
    "1. Choosing appropriate parameters for ARIMA models based on statistical criteria, such as AIC or BIC, or using automated methods, such as auto-arima.\n",
    "2. Transforming or differencing the data to make it more stationary and reduce heteroscedasticity.\n",
    "3. Decomposing the data into trend, seasonality and residual components and applying ARIMA to each component separately.\n",
    "4. Combining ARIMA with other methods, such as exponential smoothing, neural networks or machine learning algorithms, to capture nonlinear patterns and external factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb08778",
   "metadata": {
    "papermill": {
     "duration": 0.002566,
     "end_time": "2023-03-09T08:19:28.945984",
     "exception": false,
     "start_time": "2023-03-09T08:19:28.943418",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "1. **What is a p-value and how do you interpret it?:** \n",
    "A p-value is a measure of statistical significance that indicates how likely it is that an observed result occurred by chance. A low p-value (usually less than 0.05) means that the result is unlikely to occur by chance and suggests that there is a significant difference or relationship between the variables. A high p-value (usually more than 0.05) means that the result is likely to occur by chance and suggests that there is no significant difference or relationship between the variables.\n",
    "\n",
    "2. **What are some common data cleaning techniques?:** Some common data cleaning techniques are: removing duplicates, outliers, missing values, irrelevant features, etc.; correcting spelling errors, typos, formatting issues, etc.; standardizing units, scales, formats, etc.; normalizing or transforming data to reduce skewness or improve distribution; encoding categorical variables into numerical values; splitting or merging columns; etc.\n",
    "\n",
    "3. **How do you handle imbalanced data sets?:** Imbalanced data sets are those where one class or category has significantly more observations than others. This can lead to biased models that favor the majority class and ignore the minority class. Some ways to handle imbalanced data sets are: using appropriate evaluation metrics such as precision, recall, F1-score, ROC curve, etc.; oversampling or undersampling techniques such as SMOTE (Synthetic Minority Oversampling Technique), ADASYN (Adaptive Synthetic Sampling), Tomek Links (removing overlapping samples), etc.; using ensemble methods such as bagging (bootstrap aggregating), boosting (AdaBoost, XGBoost, etc.), stacking (combining multiple models), etc.; using cost-sensitive learning methods such as penalizing misclassification errors differently for different classes; using anomaly detection methods such as isolation forest\n",
    "\n",
    "4. **Tell me about a data science project you worked on recently.:** \n",
    "This question is meant to assess your experience and skills in applying data science methods to real-world problems. You should describe your project in terms of: what was the problem statement or objective; what was the data source and how did you collect and clean it; what were the tools and techniques you used for analysis and modeling; what were the results and insights you obtained; how did you communicate your findings and recommendations to stakeholders; what were the challenges and limitations you faced; what did you learn from this project; etc.\n",
    "\n",
    "5. **How do you explain technical aspects of your results to colleagues with a nontechnical background?:**\n",
    "This question is meant to assess your communication and presentation skills in conveying complex information in a simple and understandable way. You should demonstrate your ability to: use clear and concise language without jargon or acronyms; use analogies or examples to illustrate concepts or processes; use visual aids such as charts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd05656d",
   "metadata": {
    "papermill": {
     "duration": 0.002414,
     "end_time": "2023-03-09T08:19:28.951216",
     "exception": false,
     "start_time": "2023-03-09T08:19:28.948802",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "6. **What is the difference between correlation and causation?:**\n",
    "Correlation is a measure of how two variables are related or associated with each other. It can range from -1 to 1, where -1 indicates a perfect negative relationship, 0 indicates no relationship, and 1 indicates a perfect positive relationship. Causation is a stronger concept that implies that one variable causes or influences another variable. It requires establishing a causal mechanism or link between the variables, as well as ruling out other confounding factors or alternative explanations. Correlation does not imply causation, as there may be spurious or coincidental relationships between variables that are not causal.\n",
    "\n",
    "7. **What are some common machine learning algorithms and when would you use them?:** Some common machine learning algorithms are: linear regression (for predicting a continuous outcome based on one or more features); logistic regression (for predicting a binary outcome based on one or more features); k-means clustering (for grouping similar data points into clusters based on their features); decision tree (for creating a hierarchical structure of rules to classify data points based on their features); random forest (for creating an ensemble of decision trees to improve accuracy and reduce overfitting); support vector machine (for finding an optimal hyperplane that separates data points into different classes based on their features); neural network (for creating a complex nonlinear function that maps inputs to outputs using layers of artificial neurons); etc. The choice of algorithm depends on various factors such as: the type and size of data; the type and complexity of problem; the desired accuracy and speed; the available resources and constraints; etc.\n",
    "\n",
    "8. **How do you validate your model performance and avoid overfitting?:** Overfitting is a common problem in machine learning where the model learns too well from the training data and fails to generalize well to new or unseen data. To validate your model performance and avoid overfitting, you should: split your data into training, validation and test sets; use cross-validation techniques such as k-fold cross-validation to train and evaluate your model on different subsets of data; use regularization techniques such as L1 or L2 regularization to penalize complex models that have high variance; use early stopping techniques such as monitoring the validation loss or accuracy to stop training when the model starts to overfit; use ensemble methods such as bagging or boosting to combine multiple models that have low bias but high variance; etc.\n",
    "\n",
    "9. **How do you deal with missing values in your data?:** Missing values are a common issue in real-world data sets that can affect your analysis and modeling. To deal with missing values in your data, you should: first understand why they are missing (randomly or systematically) and how they affect your problem; then choose an appropriate strategy such as: deleting rows or columns with missing values if they are few and not important; imputing missing values with mean, median, mode, constant, etc. if they are numerical or categorical respectively; using advanced imputation methods such as k-nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f6b0f4",
   "metadata": {
    "papermill": {
     "duration": 0.002546,
     "end_time": "2023-03-09T08:19:28.956432",
     "exception": false,
     "start_time": "2023-03-09T08:19:28.953886",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "10. **What are some common data visualization tools and techniques?:** Data visualization is an important skill for data scientists as it helps to explore, analyze and communicate data effectively. Some common data visualization tools and techniques are: matplotlib (a Python library for creating static, animated or interactive plots); seaborn (a Python library for creating statistical graphics); ggplot2 (an R package for creating elegant and expressive graphics based on the grammar of graphics); Tableau (a software for creating interactive dashboards and reports); Power BI (a Microsoft software for creating business intelligence and analytics solutions); D3.js (a JavaScript library for manipulating documents based on data); etc. The choice of tool or technique depends on various factors such as: the type and size of data; the type and purpose of visualization; the intended audience and context; etc.\n",
    "\n",
    "11. **How do you handle ethical issues in data science?:** Ethical issues are a growing concern in data science as it involves collecting, analyzing and using large amounts of personal or sensitive data that can have significant impacts on individuals, groups or society. To handle ethical issues in data science, you should: follow the principles of fairness, accountability, transparency and privacy; respect the rights and dignity of data subjects; obtain informed consent and provide opt-out options; anonymize or encrypt personal or confidential data; ensure data quality and accuracy; avoid bias or discrimination in data collection, analysis or use; disclose any conflicts of interest or limitations of your work; adhere to relevant laws, regulations and codes of conduct; etc.\n",
    "\n",
    "12. **How do you keep yourself updated with the latest developments in data science?:** Data science is a fast-evolving field that requires constant learning and improvement. To keep yourself updated with the latest developments in data science, you should: read books, blogs, newsletters, journals, etc. related to your domain or interest; watch videos, podcasts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81932721",
   "metadata": {
    "papermill": {
     "duration": 0.00242,
     "end_time": "2023-03-09T08:19:28.961558",
     "exception": false,
     "start_time": "2023-03-09T08:19:28.959138",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "13. **How do you communicate your findings and recommendations to stakeholders?:** Communication is a key skill for data scientists as it helps to convey the value and impact of your work to stakeholders who may have different backgrounds, expectations or goals. To communicate your findings and recommendations to stakeholders, you should: understand your audience and their needs; define your objective and key message; choose an appropriate format and medium (e.g., report, presentation, dashboard, etc.); use clear and concise language; use data visualization techniques to highlight insights and patterns; provide evidence and support for your claims; address potential questions or objections; provide actionable suggestions or next steps; etc.\n",
    "\n",
    "14. **What are some of the challenges or difficulties you faced in your previous data science projects?** How did you overcome them?1: This is a behavioral question that aims to assess your problem-solving skills, resilience and learning ability. To answer this question, you should: use the STAR method (Situation, Task, Action, Result) to describe a specific example of a challenge or difficulty you faced in a previous data science project; explain what was the situation or context, what was your task or goal, what actions did you take to overcome the challenge or difficulty, and what was the result or outcome of your actions; highlight what skills or competencies you demonstrated or developed in the process; reflect on what you learned from the experience and how you applied it to future projects; etc.\n",
    "What are some of the current trends or innovations in data science that interest you? Why?1: This is an open-ended question that aims to assess your curiosity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 12.397552,
   "end_time": "2023-03-09T08:19:29.891004",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-03-09T08:19:17.493452",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
